---
title: "Final_Project"
output:
  html_document: default
  pdf_document: default
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```

<style type="text/css">
.main-container {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>

<style>
div.blue { background-color:#63a898; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">



<style>
div.blue { background-color:#63a898; border-radius: 0px; padding: 20px;}
</style>
<div class = "blue">


## [HOME](http://AR-Polyhedron.github.io/) | [ABOUT ME](http://AR-Polyhedron.github.io/About_Me/) | [CV](http://AR-Polyhedron.github.io/CV/) | [PROGRAMMING](http://AR-Polyhedron.github.io/Programming/) | [RESEARCH](http://AR-Polyhedron.github.io/Research/) 

### [Data_Course](https://github.com/AR-Polyhedron/Data_Course_RAAB) [Exam 3](http://AR-Polyhedron.github.io/Exam_3/) | [Final](http://AR-Polyhedron.github.io/Final_Project/) 

# Welcome to my descent into madness aka my Final Project

### Before we start off on this here is the initial code of tidying some of the data. Now you may be asking why isn't this just in the code? Well the initial data was 286.07mb big, even when compressing it into a zip file it was still 199mb large. So obviously these files are much to large to put on github without paying their premium service. As a college student I can't justify that cost. So I will be providing the initial code to demonstrate what I did to tidy the data into a much more manageable size along with a link to download the original dataset. The raw data can be found here.
### https://drive.google.com/open?id=1MbG2a-vQtTclHgn8-PCwIrx3CB7xzb0_ 

### Here is the code. Funnily enough this code is the third version as I had to build this whole thing from the ground up 3 times.





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(modelr)
library(broom)
library(dplyr)
library(fitdistrplus)
library(tidyr)
library(tidyverse)
library("readxl")
library(ggplot2)
```

```{r comment='',echo=FALSE}
cat(readLines('Data_Cleaning.txt'), sep = '\n')
```

```{r, echo=FALSE}
df1 <- read.csv("UL_BiologicalData_cleaned_2.csv")
df2 <- read_excel("UL2017Data.xlsx")
```

```{r, echo=FALSE}
df1 <- df1 %>% 
  rename(
    MLID = MonitoringLocationIdentifier,
    Rain.Longitude = ActivityLocation.LongitudeMeasure,
    Rain.Latitude = ActivityLocation.LatitudeMeasure,
    Rain.Start.Date = ActivityStartDate,
    Rain.End.Date = ActivityEndDate,
    Rain.Start.Time = ActivityStartTime.Time,
    Rain.End.Time = ActivityEndTime.Time,
    Rain.Depth.Feet = ActivityDepthHeightMeasure.MeasureValue
    )
```

```{r, echo=FALSE}
MLID.tag.removed <- str_remove(df1$MLID,"USGS-") 
MLID.tag.removed <- str_remove(MLID.tag.removed,"UTAHDWQ_WQX-")
MLID.tag.removed <- str_remove(MLID.tag.removed,"UTAHGS-")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NALMS-")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_CCH")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_MCD")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_HCL")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_COSL")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_CCS")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_MCL")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_HL")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_HCD")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD-TICA_BYU_CCD")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD_WQX-TICA_3P_HCL")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD_WQX-TICA_3P_MCVA")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD_WQX-TICA_TFD_AFRWS")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD_WQX-TICA_3P_MCP")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD_WQX-TICA_TFD_AFRMP")
MLID.tag.removed <- str_remove(MLID.tag.removed,"NPSWRD_WQX-TICA_TFD_SFAFC")
df1$MLID <- MLID.tag.removed
```

```{r, echo=FALSE}
Just.Time <- str_remove(df2$Time,"1899-12-31")
df2$Time <- Just.Time
df1$Rain.End.Date <- as.Date(df1$Rain.End.Date, format = "%m/%d/%Y")
df2 <- df2 %>% 
  rename(
    E.Coli.Collection.Date = Date,
    E.Coli.Collection.Time = Time,
    Collection.Location = Name
  )
```

### So first is a simple summary of the Data which looks innocent enough. Now this is a summary of only E.Coli growth. Now this seems like the best place to start off with the real introduction of to the data I was working with. So my goal of this was to combine a dataset of all of utah weather and a dataset of E.Coli growth from the field collected by my Research Mentor. All in the hopes to find a pattern with E.coli growth and rainfall. This would however prove extremely difficult.

````{r, echo=FALSE}
df2$`E. coli MPN/100ml` <- as.numeric(df2$`E. coli MPN/100ml`)
df3 <- merge(df1,df2, by=c("MLID"), all = TRUE)
summary(df3$`E. coli MPN/100ml`)
```

### Followed up with some extensive graphing of the combined dataframes. It doesn't look good.

```{r, echo=FALSE}
plot(df3$`E. coli MPN/100ml`)
ggplot(df3,aes(y=`E. coli MPN/100ml`, x=E.Coli.Collection.Date)) + geom_point()

ggplot(df3, aes(`E. coli MPN/100ml`)) + coord_cartesian(xlim = c(0, 4), ylim = c(0, 5)) + geom_density(aes(fill=factor(MLID)), alpha=0.8)

ggplot(df3,aes(y=`E. coli MPN/100ml`, x=E.Coli.Collection.Date, color=Rain.Depth.Feet)) + geom_count()

```

```{r, echo=FALSE}
ggplot(df3,aes(x=E.Coli.Collection.Date, y=`E. coli MPN/100ml`)) +
  geom_line()

ggplot(df3,aes(x=LONNAD83DD, y=`E. coli MPN/100ml`)) +geom_point()

ggplot(df3,aes(x=LATNAD83DD, y=`E. coli MPN/100ml`)) +geom_point()

```

### So I didn't even bother including the many and many plots that didn't even show anything or were so ugly they melted your eyes upon seeing them. I am also aware these are also pretty ugly but the data being shown is so ugly as well. Why polish a turd type of thing. So at this was about the point that I rebuilt the subsetting from the ground up 3 separate times. As I was continously met with a sea of NA values in every column of the Utah Weather dataset. At first I thought it was my subsetting but it turns out the Utah Weather dataset is just shite, even more shite than several initial glances. It wasn't until I was deep in this ocean of fuckery that I realized how bad it was. Even eliminating NA values did not save me as it would leave me with a desert of nothing once taken out.

```{r, echo=FALSE}
df4 <- filter(df1, df1$Rain.Start.Date == c("2016-06-01",
"2016-06-02",
"2016-06-03",
"2016-06-04",
"2016-06-05",
"2016-06-06",
"2016-06-07",
"2016-06-08",
"2016-08-09",
"2016-08-10",
"2016-08-11",
"2016-08-12",
"2016-08-13",
"2016-08-14",
"2016-08-15",
"2016-08-16",
"2016-09-16",
"2016-09-17",
"2016-09-18",
"2016-09-19",
"2016-09-20",
"2016-09-21",
"2016-09-22",
"2016-09-23",
"2016-09-26",
"2016-09-27",
"2016-09-28",
"2016-09-29",
"2016-09-30",
"2016-10-01", 
"2016-10-02",  
"2016-10-03", 
"2016-10-04", 
"2016-10-05", 
"2016-10-06", 
"2016-10-07",
"2016-10-08",
"2016-10-09",
"2016-10-10",
"2016-10-11",
"2016-10-12",
"2016-10-13",
"2016-10-14",
"2016-10-15",
"2016-10-16",
"2016-10-18",
"2016-10-19",
"2016-10-20",
"2016-10-21",
"2016-10-22",
"2016-10-23",
"2016-10-24",
"2016-10-25",
"2016-10-26",
"2016-10-27",
"2016-10-28",
"2016-10-29",
"2016-10-30",
"2016-10-31",
"2016-11-01",
"2016-11-02",
"2017-05-03",
"2017-05-04",
"2017-05-05",
"2017-05-06", 
"2017-05-07",
"2017-05-08",
"2017-05-09",
"2017-05-10",
"2017-05-11",
"2017-05-12",
"2017-05-13",
"2017-05-14",
"2017-05-15",
"2017-05-16",
"2017-06-05",
"2017-06-06",
"2017-06-07",
"2017-06-08",
"2017-06-09",
"2017-06-10",
"2017-06-11",
"2017-06-12",
"2017-06-13",
"2017-06-14",
"2017-06-15",
"2017-06-27",
"2017-06-28",
"2017-06-29",
"2017-06-30",
"2017-07-01",
"2017-07-02",
"2017-07-03",
"2017-07-04",
"2017-07-06",
"2017-07-07",
"2017-07-08",
"2017-07-09",
"2017-07-10",
"2017-07-11",
"2017-07-12",
"2017-07-13",
"2017-07-05",
"2017-07-28",
"2017-07-29",
"2017-07-30",
"2017-07-31",
"2017-08-01",
"2017-08-02",
"2017-08-03",
"2017-08-07",
"2017-08-08",
"2017-08-09",
"2017-08-10",
"2017-08-11",
"2017-08-12",
"2017-08-13",
"2017-08-14",
"2017-08-15",
"2017-08-16",
"2017-08-17",
"2017-08-18",
"2017-08-19",
"2017-08-20",
"2017-08-21",
"2017-08-22",
"2017-08-23",
"2017-08-24",
"2016-07-20",
"2016-07-21",
"2016-07-22",
"2016-07-23",
"2016-07-24",
"2016-07-25",
"2016-07-26",
"2016-07-27"))
```

### So one last effort was made to subset the data based on the times  E coli was taken, hoping to achieve a small but reliable set of data by manually(look at the code) filtering out the dates except everything a week leading up to the collection date.

```{r, echo=FALSE}
df5 <- merge(df4,df2, by=c("MLID"), all = TRUE)
summary(df5)
```

```{r}
plot(df5$`E. coli MPN/100ml`)
```

```{r}
ggplot(df5,aes(x=E.Coli.Collection.Date, y=`E. coli MPN/100ml`)) + geom_line()
```
### It didn't work. In fact it looks the same.So at this point I had clearly hit the realization that it was completely unworkable or I was complete shit at coding. Now I won't disagree I am still very much an ameteur, however I'm pretty confidant the Utah Weather dataset is 100% fuck. The other one also needs some work but I don't know who collected it specifically so I got to be polite. I also remember early on that Dr. Zahn had said sometimes datasets are so poorly recorded we can't do anything with them and that he has had that experience several times. So not disheartening, I just pick myself up and start over again with a new dataset...however this conclusion was reached 4/27/2020 at 1am...so I chose to just grind my face against the grindstone and at least demonstrate I knew some of the skills we learned in class. So I moved to modeling.

```{r}
mod1.0 <- aov(`E. coli MPN/100ml` ~ Rain.Start.Date, data = df3)
mod1.1 <- aov(`E. coli MPN/100ml` ~ MLID, data = df3)

mod1.2 <- lm(`E. coli MPN/100ml` ~ Rain.Start.Date, data = df3)
mod1.3 <- lm(`E. coli MPN/100ml` ~ MLID, data = df3)
mod1.4 <- lm(`E. coli MPN/100ml` ~ E.Coli.Collection.Date, data = df3)

mod2.0 <- aov(`E. coli MPN/100ml` ~ MLID, data = df5)
mod2.1 <- lm(`E. coli MPN/100ml` ~ MLID, data = df5)

mod1.7 <- lm(`E. coli MPN/100ml` ~ MLID: E.Coli.Collection.Date, data = df3)
```

```{r}
summary(mod1.0)
summary(mod1.1)
summary(mod1.2)
summary(mod1.3)
summary(mod1.4)
summary(mod1.7)
summary(mod2.0)
summary(mod2.1)
```

```{r}
plot(df3$`E. coli MPN/100ml`~ df3$Rain.Start.Date)
abline(mod1.0, col="black")
plot(df3$`E. coli MPN/100ml`~ df3$MLID)
abline(mod1.1, col="blue")
plot(df3$`E. coli MPN/100ml`~ df3$Rain.Start.Date)
abline(mod1.2, col="red")
plot(df3$`E. coli MPN/100ml`~ df3$MLID)
abline(mod1.3, col="green")
plot(df3$`E. coli MPN/100ml`~ df3$E.Coli.Collection.Date)
abline(mod1.4, col="orange")
plot(df3$`E. coli MPN/100ml`~ df3$E.Coli.Collection.Date)
abline(mod1.7, col="yellow")

```

```{r}
plot(df5$`E. coli MPN/100ml`~ df5$MLID)
abline(mod2.0, col="black")
abline(mod2.1, col="blue")
```

```{r}
plot(df3$`E. coli MPN/100ml`)
abline(mod1.0, col="black")
abline(mod1.1, col="blue")
abline(mod1.4, col="green")
abline(mod1.7, col="orange")
```

```{r}
plot(df5$`E. coli MPN/100ml`)
abline(mod2.0, col="black")
abline(mod2.1, col="blue")
abline(mod1.7, col="red")
```

### Hmmmm yup those models are shit but they don't really got much to work on and I suck at modelling anyway. Time to move on to looking at residuals and adding predictions.

```{r}
mean(mod1.4$residuals^2)
mean(mod1.7$residuals^2)
```

### Well model 1.7 it is hmmm what's this an error on the following code for predictions.

#preds <- add_predictions(df3, mod1.7) 
#preds[1:11:13,c(3350,3400)] 

### Worry not I will just look up error and see how to fix it, not like a lot of that has happened during this project at aaaaaaaaaaaaaaalllllllllllllllll.
### https://stackoverflow.com/questions/45784594/error-in-model-frame-defaultterms-newdata-na-action-na-action-xlev-objec

### I could try to do that but oh look at that it's 10:30pm on 4/27/2020 and I have a Molecular Biology final and an Organic Chemistry final over the next couple days and haven't studied for. So I'm calling it quits on this. 

### So I will probably get a poor grade on this final however I will plead my case a little. Yes I admit that I should have looked in more depth in my planned final data sets so that I could have started over without it being a problem. That is totally on me. However I feel that given what I was working with I did the best that anyone could have done and still demostrated the skills we learned over the course, yes my modeling portion can use some work overall but it was still there. I think this at least a low B grade especially considering the amount of work that was just not included for not working and what is working is decently meaty. If you disagree I challenge anyone to try and cobble together less of a mess analysis with these two datat sets. 

### Now for my future in programming with R, I definetly want to continue it and fully known there are many basics I need to shore up. I will be taking a Stats class this summer and plan on using R on my class work and the class to help my understanding of modeling data. I also want to take another crack at the final project with a much better data set and I hope to work with Dr. Zahn with this throughout the summer.
